# This assumes 'tickets' is character. Adjust the gsub pattern as needed for other non-numeric characters
ticketing$tickets <- gsub(",", "", ticketing$tickets) # Remove commas
ticketing$tickets <- as.numeric(ticketing$tickets) # Convert to numeric
hist(ticketing$Tickets, main = "Distribution of Tickets", xlab = "Tickets", col = "blue")
median(ticketing$Tickets)
mean(ticketing$Tickets)
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path= path_to_data, pattern="*.csv", full.names = TRUE)
# Read all CSV files and combine them into a single data frame
schedule_list <- lapply(file_list, read.csv)
schedule <- do.call(rbind, schedule_list)
# Set the path to the directory containing the CSV files
path_to_data <- "data_sales/"
# List all CSV files in the directory
file_list <- list.files(path= path_to_data, pattern="*.csv", full.names = TRUE)
# Read all CSV files and combine them into a single data frame
sales_list <- lapply(file_list, read.csv)
sales <- do.call(rbind, schedule_list)
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
schedule_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(schedule_list, names))
# Keep only the common columns in each data frame and combine
schedule_list <- lapply(schedule_list, `[`, common_columns)
schedule <- do.call(rbind, schedule_list)
# Check the combined data frame
print(schedule)
schedule <- read.csv('data_schedule/2022_Schedule_info.csv')
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
schedule_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(schedule_list, names))
# Keep only the common columns in each data frame and combine
schedule_list <- lapply(schedule_list, `[`, common_columns)
schedule <- do.call(rbind, schedule_list)
# Check the combined data frame
print(schedule)
schedule <- read.csv('data_schedule/2022_Schedule_info.csv')
# Set the path to the directory containing the CSV files
path_to_data <- "data_sales/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
schedule_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(schedule_list, names))
# Keep only the common columns in each data frame and combine
schedule_list <- lapply(schedule_list, `[`, common_columns)
schedule <- do.call(rbind, schedule_list)
# Check the combined data frame
print(schedule)
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
schedule_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(schedule_list, names))
# Keep only the common columns in each data frame and combine
schedule_list <- lapply(schedule_list, `[`, common_columns)
schedule <- do.call(rbind, schedule_list)
# Check the combined data frame
print(schedule)
# Set the path to the directory containing the CSV files
path_to_data <- "data_sales/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
sales_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(sales_list, names))
# Keep only the common columns in each data frame and combine
sales_list <- lapply(sales_list, `[`, common_columns)
sales <- do.call(rbind, sales_list)
# Check the combined data frame
print(sales)
schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Read the column names of each CSV file
column_names_list <- lapply(file_list, function(file) {
cols <- names(read.csv(file, nrows = 1)) # Read only the first row for efficiency
return(setNames(object = cols, nm = basename(file))) # Set the file name as the name of the list element
})
# Find the unique set of column names across all files
all_columns <- unique(unlist(column_names_list))
# Identify the missing column in each file
missing_columns <- lapply(column_names_list, function(cols) setdiff(all_columns, cols))
# Print out the missing columns along with the file names
print(missing_columns)
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Read the column names of each CSV file
column_names_list <- lapply(file_list, function(file) {
cols <- names(read.csv(file, nrows = 1)) # Read only the first row for efficiency
return(setNames(object = cols, nm = basename(file))) # Set the file name as the name of the list element
})
# Find the unique set of column names across all files
all_columns <- unique(unlist(column_names_list))
# Identify the missing column in each file
missing_columns <- lapply(column_names_list, function(cols) setdiff(all_columns, cols))
# Print out the missing columns along with the file names
print(missing_columns)
# Identify columns that are not present in all files
non_common_columns <- Reduce(function(x, y) setdiff(union(x, y), intersect(x, y)), column_names_list)
# Print out the non-common columns
print(non_common_columns)
schedule_12 <- read.csv('data_schedule/2017_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2018_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2019_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2020_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2017_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2018_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2019_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2021_Schedule_info.csv')
schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
str(df)
summary(df)
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
# Set the path to the directory containing the CSV files
path_to_data <- "data_schedule/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
schedule_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(schedule_list, names))
# Keep only the common columns in each data frame and combine
schedule_list <- lapply(schedule_list, `[`, common_columns)
schedule <- do.call(rbind, schedule_list)
schedule$source_file <- NULL
# Check the combined data frame
print(schedule)
# Set the path to the directory containing the CSV files
path_to_data <- "data_sales/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
sales_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(sales_list, names))
# Keep only the common columns in each data frame and combine
sales_list <- lapply(sales_list, `[`, common_columns)
sales <- do.call(rbind, sales_list)
# Check the combined data frame
print(sales)
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
str(df)
summary(df)
sales
# Set the path to the directory containing the CSV files
path_to_data <- "data_sales/"
# List all CSV files in the directory
file_list <- list.files(path = path_to_data, pattern = "*.csv", full.names = TRUE)
# Define a function to read and process each CSV file
process_csv <- function(file_name) {
df <- read.csv(file_name)
# Optionally, add a source file column to identify the data's origin
df$source_file <- basename(file_name)
return(df)
}
# Read all CSV files into a list of data frames
sales_list <- lapply(file_list, process_csv)
# Find the intersection of all column names across all data frames
common_columns <- Reduce(intersect, lapply(sales_list, names))
# Keep only the common columns in each data frame and combine
sales_list <- lapply(sales_list, `[`, common_columns)
sales <- do.call(rbind, sales_list)
sales$source_file <- NULL
# Check the combined data frame
print(sales)
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
str(df)
summary(df)
ticketing$tickets <- gsub(",", "", ticketing$tickets) # Remove commas
sales$tickets <- gsub(",", "", sales$tickets) # Remove commas
sales$tickets <- gsub(",", "", sales$tickets) # Remove commas
sales$tickets <- as.numeric(sales$sales) # Convert to numeric
hist(sales$Tickets, main = "Distribution of Tickets", xlab = "Tickets", col = "blue")
source("C:/Users/deepg/Documents/classes_local/sports_analytics/classwork/braves_pricing_2024/code/simulated_forecasts.R", echo=TRUE)
source("code/setup.R")
#-------------------------------------------------------------------------------
# Load data sets
#-------------------------------------------------------------------------------
past_seasons <- vroom::vroom('data/top_down_season_data.csv')
data_2025    <- vroom::vroom('data/top_down_future_data.csv')
#-------------------------------------------------------------------------------
# Prep data
#-------------------------------------------------------------------------------
past_seasons <- as.data.frame(past_seasons)  %>%
janitor::clean_names()                     %>%
na.omit()                                  %>%
select(-date)
data_2025 <- as.data.frame(data_2025)    %>%
janitor::clean_names()                 %>%
na.omit()                              %>%
select(-date)
#-------------------------------------------------------------------------------
# Explore the data
#-------------------------------------------------------------------------------
x_label  <- ('Ticket Sales')
y_label  <- ('Density')
title    <- ('Ticket Sales Distribution')
legend   <- ('')
past_seasons                                        %>%
ggplot2::ggplot(
aes(x = ticket_sales,
color = factor(season)))                      +
# facet_grid(.~ season)                             +
geom_density(size = 1.2)                            +
geom_rug(color = 'steelblue4')                      +
scale_y_continuous(label = scales::comma)           +
scale_x_continuous(label = scales::comma)           +
scale_color_manual(legend, values = palette)        +
xlab(x_label)                                       +
ylab(y_label)                                       +
ggtitle(title)                                      +
graphics_theme_1
ggsave(
'images/ticket_sales_distributions.png',
plot = last_plot(),
device = NULL,
scale = 1,
width = 8,
height = 6,
dpi = 300
)
#-------------------------------------------------------------------------------
# Predict Sales with boosted model
#-------------------------------------------------------------------------------
# prep for parallel processing
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
#-------------------------------------------------------------------------------
# hyperparameter tuning
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf,
resamples = model_cv_folds,
grid = xgboost_grid,
metrics = yardstick::metric_set(rmse, rsq, mae),
control = tune::control_grid(verbose = TRUE)
)
#-------------------------------------------------------------------------------
xgboost_tuned %>%
tune::show_best(metric = "rmse") %>%
knitr::kable()
#-------------------------------------------------------------------------------
xgboost_best_params <- xgboost_tuned %>%
tune::select_best("rmse")
knitr::kable(xgboost_best_params)
#-------------------------------------------------------------------------------
xgboost_model_final <- xgboost_model %>%
finalize_model(xgboost_best_params)
#-------------------------------------------------------------------------------
train_processed  <- bake(preprocessing_recipe,  new_data = training(model_split))
train_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = ticket_sales ~ .,
data    = train_processed
) %>%
# predict the sale prices for the training data
predict(new_data = train_processed) %>%
bind_cols(training(model_split))
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(ticket_sales, .pred) %>%
mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
#-------------------------------------------------------------------------------
test_processed  <- bake(preprocessing_recipe, new_data = testing(model_split))
test_prediction <- xgboost_model_final %>%
# fit the model on all the training data
fit(
formula = ticket_sales ~ .,
data    = train_processed
) %>%
# use the training model fit to predict the test data
predict(new_data = test_processed) %>%
bind_cols(testing(model_split))
# measure the accuracy of our model using `yardstick`
xgboost_score <-
test_prediction %>%
yardstick::metrics(ticket_sales, .pred) %>%
mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score)
#-------------------------------------------------------------------------------
# Visualize the residuals
#-------------------------------------------------------------------------------
final_prediction_residual <- test_prediction %>%
arrange(.pred) %>%
mutate(residual_pct = (ticket_sales - .pred) / .pred)
ggplot(final_prediction_residual, aes(x = .pred, y = residual_pct)) +
geom_point(alpha = .9,color= 'steelblue4')                        +
xlab("Ticket Sales Prediction")                                   +
ylab("Residual (%)")                                              +
scale_x_continuous(labels = scales::comma)                        +
scale_y_continuous(labels = scales::percent)                      +
graphics_theme_1                                                  +
labs(x="Tickets Prediction",
y="Residual (%)",
title = "Tickets Prediction and Residual Error",
subtitle = "",
caption  = "[note]")                                         +
theme(plot.caption = element_text(hjust = 0,
face= "italic",
color = "grey10"),
plot.title.position = "plot",
plot.caption.position =  "plot",
plot.subtitle = element_text(color = "grey10"))
ggsave(
'images/xgboost_error_actual.png',
plot = last_plot(),
device = NULL,
scale = 1,
width = 8,
height = 6,
dpi = 300
)
#-------------------------------------------------------------------------------
# Separate data sets for other models.
#-------------------------------------------------------------------------------
train <- as.data.frame(training(model_split))
test  <- as.data.frame(testing(model_split))
#-------------------------------------------------------------------------------
# Attach boosted estimates to test data set
#-------------------------------------------------------------------------------
# Apply gradient boosting predictions
xgb_preds <- xgboost_model_final %>%
fit(
formula = ticket_sales ~ .,
data    = train_processed
)                              %>%
predict(new_data = test)
test$.pred_xgb <- xgb_preds$.pred
#-------------------------------------------------------------------------------
# Load data sets
#-------------------------------------------------------------------------------
past_seasons <- vroom::vroom('data/top_down_season_data.csv')
data_2025    <- vroom::vroom('data/top_down_future_data.csv')
# Assuming 'data' is your dataframe
set.seed(123) # For reproducibilit/y
split <- sample.split(data$Tickets, SplitRatio = 0.8)
install.packages("caTools")
library(caTools)
library(caTools)
library(caTools)
library(caTools)
install.packages("bitops")
library(caTools)
library(caTools)
# Assuming 'data' is your dataframe
set.seed(123) # For reproducibilit/y
split <- sample.split(data$Tickets, SplitRatio = 0.8)
library(caTools)
# Assuming 'data' is your dataframe
set.seed(123) # For reproducibilit/y
split <- sample.split(df$Tickets, SplitRatio = 0.8)
train <- subset(df, split == TRUE)
test <- subset(df, split == FALSE)
# Prepare the data for XGBoost
train_xgb <- as.matrix(train[, -which(names(train) == "Tickets")])
train_y <- train$Tickets
test_xgb <- as.matrix(test[, -which(names(test) == "Tickets")])
test_y <- test$Tickets
library(randomForest)
rf_model <- randomForest(Tickets ~ ., data = train)
train
library(randomForest)
rf_model <- randomForest(Tickets ~ ., data = train)
train_complete <- na.omit(train)
train <- subset(df, split == TRUE)
train_complete <- na.omit(train)
rf_model <- randomForest(Tickets ~ ., data = train)
train_complete <- na.omit(train)
View(train_complete)
rf_model <- randomForest(Tickets ~ ., data = train_complete)
summary(rf_model)
rf_predictions <- predict(rf_model, test)
rf_predictions <- predict(rf_model, test)
print(paste("RF MAE:", mae_rf))
mae_rf <- mae(test$Tickets, rf_predictions)
install.packages("Metrics")
library(randomForest)
library(Metrics)
library(randomForest)
library(Metrics)
install.packages("Metrics")
options(repos = c(CRAN = "https://cran.r-project.org"))
install.packages("Metrics")
library(randomForest)
library(Metrics)
train_complete <- na.omit(train)
rf_model <- randomForest(Tickets ~ ., data = train_complete)
summary(rf_model)
rf_predictions <- predict(rf_model, test)
mae_rf <- mae(test$Tickets, rf_predictions)
print(paste("RF MAE:", mae_rf))
# XGBoost
library(xgboost)
install.packages("xgboost")
# XGBoost
library(xgboost)
# Prepare data for xgboost
dtrain <- xgb.DMatrix(data = as.matrix(train[,-which(names(train) == "Tickets")]), label = train$Tickets)
load("C:/Users/deepg/Documents/classes_local/deep_learning/dl_homework/lab1/.RData")
library(dplyr)
#schedule_12 <- read.csv('data_schedule/2022_Schedule_info.csv')
schedule$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
#ticketing <- read.csv('data_sales/2022_Ticketing_Data.csv')
sales$Date <- as.Date(sales$Date, format = "%Y-%m-%d")
df <- left_join(sales, schedule,by = "Date")
write.csv(df,"combined_data.csv")
# Predictions
rf_predictions <- predict(rf_model, newdata = test)
df_test_sc <- read.csv('prediction_data/2023_Schedule_info.csv')
df_test_ti <- read.csv('prediction_data/2023_Ticketing_Data.csv')
df_test_sc <- read.csv('prediction_data/2023_Schedule_info.csv')
df_test_ti <- read.csv('prediction_data/2023_Ticketing_Data.csv')
df_test_sc$Date <- as.Date(schedule$Date, format = "%m/%d/%Y")
df_test_sc <- read.csv('prediction_data/2023_Schedule_info.csv')
df_test_ti <- read.csv('prediction_data/2023_Ticketing_Data.csv')
df_test_sc$Date <- as.Date(df_test_sc$Date, format = "%m/%d/%Y")
df_test_ti$Date <- as.Date(df_test_ti$Date, format = "%Y-%m-%d")
df <- left_join(df_test_ti, df_test_sc,by = "Date")
write.csv(df,"combined_test_data.csv")
